{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Jupyter Notebooks for the visual analysis of critical choices in Global Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valentina Noacco, Andres Peñuela-Fernandez, Francesca Pianosi, Thorsten Wagener \n",
    "### (University of Bristol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document provides:\n",
    "* a brief introduction to Global Sensitivity Analysis (GSA);\n",
    "* a workflow to asssess one of the critical choices needed to set up a GSA application. Here we use the SAFE (Sensitivity Analysis For Everybody) toolbox ([References](#References) 1-2).\n",
    "\n",
    "In this example we apply the Regional Sensitivity Analysis GSA method to the rainfall-runoff Hymod model.\n",
    "\n",
    "\n",
    "# PART I: Introduction\n",
    "\n",
    "## What is Global Sensitivity Analysis and why shall we use it?\n",
    "\n",
    "**Global Sensitivity Analysis** is a set of mathematical techniques which investigate how the uncertainty in the model inputs influences the variability of the model outputs ([References](#References) 3-4).\n",
    "\n",
    "The benefits of applying GSA are:\n",
    "\n",
    "* **Better understanding of the model**: Evaluate the model behaviour beyond default set-up\n",
    "    \n",
    "* **“Sanity check” of the model**: Test whether the model \"behaves well\" (model validation)\n",
    "    \n",
    "* **Priorities for uncertainty reduction**: Identify the important inputs on which to focus computationally-intensive calibration, acquisition of new data, etc. \n",
    "    \n",
    "* **More transparent and robust decisions**: Understand how assumptions about uncertain inputs reflect on the modelling outcome and thus on model-informed decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Global Sensitivity Analysis works\n",
    "\n",
    "GSA investigates how the uncertainty in the model input factors influences the variability of the model output.\n",
    "\n",
    "An '**input factor**' is any element that can be changed before running the model. In general, input factors could be the model's parameters, forcing input data, but also the very equations implemented in the model or other set-up choices (for instance, the spatial resolution) needed for the model execution on a computer.\n",
    "\n",
    "An '**output**' is any variable that is obtained after the model execution.\n",
    "\n",
    "The main steps to perform GSA are summarised in the figure below.\n",
    "<img src=\"how_GSA_works.png\" width=\"800px\">\n",
    "a. The input factors are sampled from their ranges of variability.\n",
    "\n",
    "b. The model is repeatedly run against each of the input sampled combinations.\n",
    "\n",
    "c. The output samples so obtained can be used to characterise the output uncertainty, for instance through a probability distribution or scatter plots.\n",
    "\n",
    "d. GSA is applied to the input and output samples in order to obtain a set of sensitivity indices. The sensitivity indices measure the relative influence of each input factor on the uncertainty of the output ([References](#References) 4,5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Workflow application\n",
    "### Step 1: Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, absolute_import, print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# Import SAFE modules:\n",
    "import os\n",
    "mydir = \"C:/Users/valen/OneDrive - University of Bristol/proj_SAFEVAL/SAFEPy/SAFEpython_v0.0.0\"\n",
    "os.chdir(mydir + \"/SAFEpython\")\n",
    "\n",
    "import SAFEpython.RSA_groups as Rg # module to perform RSA with groups\n",
    "import SAFEpython.plot_functions as pf # module to visualize the results\n",
    "from SAFEpython.model_execution import model_execution # module to execute the model\n",
    "from SAFEpython.sampling import AAT_sampling, AAT_sampling_extend # module to perform the input sampling\n",
    "from SAFEpython.util import aggregate_boot  # function to aggregate the bootstrap results\n",
    "from SAFEpython import HyMod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model: Rainfall-runoff Hymod model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying GSA, let's have a brief overview of the HyMod model, which it is a parsimonious rainfall-runoff model based on the theory of runoff yeild under infiltration excess. \n",
    "\n",
    "Hymod (Boyle 2001; Wagener et al. 2001) is composed of a soil moisture accounting routine, and a flow routing routine, which in its turn is composed of a fast and a slow routing pathway.\n",
    "\n",
    "The model structure is illustrated in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left><img src=\"Hymod_scheme.png\" width=\"700px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following steps will be performed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Setup the model\n",
    "\n",
    "Define: \n",
    "- the input factors whose influence is to be analysed with GSA, \n",
    "- their range of variability (choice made by expert judgement, available data or previous studies),\n",
    "- choice of their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 5 # number of input factors\n",
    "N = 2000 # number of samples\n",
    "distr_par = [np.nan] * M\n",
    "col_names = [\"$S_M$\" , \"$beta$\" , \"$alpha$\", \"$R_s$\", \"$R_F$\"] # input factors of interest\n",
    "distr_fun = st.uniform # uniform distribution\n",
    "samp_strat = 'lhs' # Latin Hypercube\n",
    "fun_test = HyMod.hymod_MulObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unit</th>\n",
       "      <th>Min value</th>\n",
       "      <th>Max value</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>$S_M$</td>\n",
       "      <td>mm</td>\n",
       "      <td>1.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>Maximum storage capacity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>$beta$</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Degree of spatial variability of the $S_M$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>$alpha$</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Factor distributing slow and quick flows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>$R_s$</td>\n",
       "      <td>-</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Fractional discharge of the slow release reser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>$R_F$</td>\n",
       "      <td>-</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fractional discharge of the quick release rese...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unit  Min value  Max value  \\\n",
       "$S_M$     mm        1.0      400.0   \n",
       "$beta$     -        0.0        2.0   \n",
       "$alpha$    -        0.0        1.0   \n",
       "$R_s$      -        0.0        0.1   \n",
       "$R_F$      -        0.1        1.0   \n",
       "\n",
       "                                               Description  \n",
       "$S_M$                             Maximum storage capacity  \n",
       "$beta$          Degree of spatial variability of the $S_M$  \n",
       "$alpha$           Factor distributing slow and quick flows  \n",
       "$R_s$    Fractional discharge of the slow release reser...  \n",
       "$R_F$    Fractional discharge of the quick release rese...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# range of variability\n",
    "data = [[\"mm\"  , 1   , 400 , \"Maximum storage capacity\"],\n",
    "        [\"-\"   , 0   , 2   , \"Degree of spatial variability of the $S_M$\"],\n",
    "        [\"-\"   , 0, 1, \"Factor distributing slow and quick flows\"],\n",
    "        [\"-\", 0, 0.1, \"Fractional discharge of the slow release reservoir\"],\n",
    "        [\"-\", 0.1, 1, \"Fractional discharge of the quick release reservoirs\"]]\n",
    "model_inputs = pd.DataFrame(data, \n",
    "                           columns=[\"Unit\", \"Min value\", \"Max value\", \"Description\"],\n",
    "                           index = [\"$S_M$\" , \"$beta$\" , \"$alpha$\", \"$R_s$\", \"$R_F$\"])\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "data = np.genfromtxt(mydir +'/data/LeafCatch.txt', comments='%')\n",
    "rain = data[0:365, 0] # 1-year simulation\n",
    "evap = data[0:365, 1]\n",
    "flow = data[0:365, 2]\n",
    "warmup = 30 # Model warmup period (days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class setup_model:\n",
    "    def __init__(self, x1min, x2min, x3min, x4min, x5min, x1max, x2max, x3max, x4max, x5max):\n",
    "        # The shape parameters of the uniform distribution are the lower limit and the difference between lower and upper limits:\n",
    "        self.xmin = [x1min.value, x2min.value, x3min.value, x4min.value, x5min.value]\n",
    "        self.xmax = [x1max.value, x2max.value, x3max.value, x4max.value, x5max.value]\n",
    "        for i in range(M):\n",
    "            distr_par[i] = [self.xmin[i], self.xmax[i] - self.xmin[i]]\n",
    "        self.distr_par = distr_par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Sample inputs space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of model runs (N) typically increases proportionally to the number of input factors ($M$) and will depend on the GSA methods chosen as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sample_input:\n",
    "    def __init__(self,distr_par):\n",
    "        self.X = AAT_sampling(samp_strat, M, distr_fun, distr_par, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sampled input factors combination, we run the model and save the associated model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class run_model:\n",
    "    def __init__(self,X):\n",
    "        self.Y = model_execution(fun_test, X, rain, evap, flow, warmup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Apply Sensitivity Analysis (RSA) method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSA requires to sort the output samples and then to split them into a certain number of groups (defined by the user). Afterwards, RSA identifies the sub-samples in the inputs space that produced the outputs in each group and compute the cumulative distribution function (CDF) of each sub-sample. Finally, the sensitivity indices are defined as the (mean) maximum vertical distance between the CDFs of the various groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we divide the output samples into 10 groups, where each group contains the same number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSA_method:\n",
    "    def __init__(self,X,Y): \n",
    "        mvd_median, mvd_mean, mvd_max, spread_median, spread_mean, spread_max, self.idx, self.Yk = \\\n",
    "        Rg.RSA_indices_groups(X, Y, ngroup=10, Nboot=0)\n",
    "\n",
    "        self.mvd_p = pd.DataFrame(mvd_median,columns=[\"y\"])\n",
    "        Xdf = pd.DataFrame(X,columns=[col_names])\n",
    "        xx = pd.melt(Xdf)\n",
    "        Ydf = pd.DataFrame(Y,columns=[\"y\"])\n",
    "        yy = pd.concat([Ydf]*M,ignore_index=True)\n",
    "        self.new_data = pd.concat([xx,yy],axis=1)\n",
    "        self.new_data.columns = [\"Input\",\"x\",\"y\"]\n",
    "        all_ind = np.concatenate((self.idx,self.idx,self.idx,self.idx,self.idx),axis=0)\n",
    "        all_ind = pd.DataFrame(all_ind,columns=['idx'])\n",
    "        self.new_data['idx'] = all_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Check model behaviour by visualising input/output samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplots are plotted to visualise the behaviour of the output over each input factor in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of interactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_figures(change):\n",
    "    with fig.batch_update():\n",
    "        distr_par = setup_model(x1min, x2min, x3min, x4min, x5min, x1max, x2max, x3max, x4max, x5max).distr_par\n",
    "        X = sample_input(distr_par).X\n",
    "        Y = run_model(X).Y[:,0]\n",
    "        new_data = RSA_method(X,Y).new_data\n",
    "        fig.data[0].x = new_data.loc[new_data['Input'] == 'x1', 'x']    \n",
    "        fig.data[1].x = new_data.loc[new_data['Input'] == 'x2', 'x']    \n",
    "        fig.data[2].x = new_data.loc[new_data['Input'] == 'x3', 'x']\n",
    "        fig.data[3].x = new_data.loc[new_data['Input'] == 'x4', 'x']\n",
    "        fig.data[4].x = new_data.loc[new_data['Input'] == 'x5', 'x']\n",
    "        fig.data[0].marker.color = new_data.loc[new_data['Input'] == 'x1', 'idx']\n",
    "        fig.data[1].marker.color = new_data.loc[new_data['Input'] == 'x2', 'idx']    \n",
    "        fig.data[2].marker.color = new_data.loc[new_data['Input'] == 'x3', 'idx']\n",
    "        fig.data[3].marker.color = new_data.loc[new_data['Input'] == 'x4', 'idx']\n",
    "        fig.data[4].marker.color = new_data.loc[new_data['Input'] == 'x5', 'idx']\n",
    "        fig.data[0].y = new_data.loc[new_data['Input'] == 'x1', 'y']    \n",
    "        fig.data[1].y = new_data.loc[new_data['Input'] == 'x2', 'y']    \n",
    "        fig.data[2].y = new_data.loc[new_data['Input'] == 'x3', 'y']\n",
    "        fig.data[3].y = new_data.loc[new_data['Input'] == 'x4', 'y']\n",
    "        fig.data[4].y = new_data.loc[new_data['Input'] == 'x5', 'y']\n",
    "        fig.layout.barmode = 'overlay'\n",
    "        #fig.update_xaxes(matches=None)\n",
    "\n",
    "        mvd_p = RSA_method(X,Y).mvd_p\n",
    "        for i in range(0,5):\n",
    "            fig1.data[i].y = np.array(mvd_p.y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1min = widgets.FloatSlider(min=0,max=99,value=50, description = 'x1 min: ', continuous_update=False)\n",
    "x1min.observe(update_figures,names = 'value')\n",
    "x1max = widgets.FloatSlider(min=100,max=200,value=150, description = 'x1 max: ', continuous_update=False)\n",
    "x1max.observe(update_figures,names = 'value')\n",
    "\n",
    "x2min = widgets.FloatSlider(min=0,max=0.9,value=0.5, description = 'x2 min: ', continuous_update=False)\n",
    "x2min.observe(update_figures,names = 'value')\n",
    "x2max = widgets.FloatSlider(min=1,max=2,value=1.5, description = 'x2 max: ', continuous_update=False)\n",
    "x2max.observe(update_figures,names = 'value')\n",
    "\n",
    "x3min = widgets.FloatSlider(min=0,max=0.4,value=0.2, description = 'x3 min: ', continuous_update=False)\n",
    "x3min.observe(update_figures,names = 'value')\n",
    "x3max = widgets.FloatSlider(min=0.5,max=1,value=0.8, description = 'x3 max: ', continuous_update=False)\n",
    "x3max.observe(update_figures,names = 'value')\n",
    "\n",
    "x4min = widgets.FloatSlider(min=0,max=0.04,value=0.02, description = 'x4 min: ', continuous_update=False)\n",
    "x4min.observe(update_figures,names = 'value')\n",
    "x4max = widgets.FloatSlider(min=0.05,max=0.1,value=0.08, description = 'x4 max: ', continuous_update=False)\n",
    "x4max.observe(update_figures,names = 'value')\n",
    "\n",
    "x5min = widgets.FloatSlider(min=0.1,max=0.4,value=0.2, description = 'x5 min: ', continuous_update=False)\n",
    "x5min.observe(update_figures,names = 'value')\n",
    "x5max = widgets.FloatSlider(min=0.5,max=1,value=0.8, description = 'x5 max: ', continuous_update=False)\n",
    "x5max.observe(update_figures,names = 'value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2de08b2ae570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdistr_par\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msetup_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx3min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx4min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx5min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx2max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx3max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx4max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx5max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistr_par\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistr_par\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRSA_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m p = px.scatter(new_data, x = \"x\", y = 'y', facet_col = \"Input\",color=\"idx\", color_continuous_scale = px.colors.diverging.RdYlBu[::-1],\n",
      "\u001b[1;32m<ipython-input-8-aeee6fdbbc16>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_execution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rain' is not defined"
     ]
    }
   ],
   "source": [
    "distr_par = setup_model(x1min, x2min, x3min, x4min, x5min, x1max, x2max, x3max, x4max, x5max).distr_par\n",
    "X = sample_input(distr_par).X\n",
    "Y = run_model(X).Y[:,0]\n",
    "new_data = RSA_method(X,Y).new_data\n",
    "p = px.scatter(new_data, x = \"x\", y = 'y', facet_col = \"Input\",color=\"idx\", color_continuous_scale = px.colors.diverging.RdYlBu[::-1],\n",
    "               width=1000, height=500)\n",
    "fig = go.FigureWidget(data=p, layout=go.Layout(barmode='overlay'))\n",
    "fig.update_xaxes(title_text = \"\")\n",
    "fig.update_xaxes(matches=None)\n",
    "fig.layout.yaxis.title=\"Y\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Plot sensitivity indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitivity indices for the RSA method are the maximum vertical distances over each pair of CDF.\n",
    "\n",
    "Definition of the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85153c45fd3349ae9d98176b837d6e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'hoverlabel': {'namelength': 0},\n",
       "              'hovertemplate': 'color=red<br>x=%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mvd_p = RSA_method(X,Y).mvd_p\n",
    "p1 = px.scatter(mvd_p, x = [\"$S_M$\" , \"$beta$\" , \"$alpha$\", \"$R_s$\", \"$R_F$\"], y = 'y',color=['red','blue','green',\"magenta\",\"cyan\"], width=600, height=300)\n",
    "\n",
    "fig1  = go.FigureWidget(data=p1,layout=go.Layout(barmode='overlay'))\n",
    "fig1.layout.yaxis.range=[0,1]\n",
    "fig1.layout.yaxis.title=\"Maximum Vertical Distance\"\n",
    "fig1.layout.xaxis.title=\"\"\n",
    "fig1.update_traces(marker=dict(size=20),showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the interactive figures + sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74533f5ed39a4a378afb088fb673b2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(FloatSlider(value=50.0, continuous_update=False, description='x1 …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.VBox([widgets.HBox([widgets.VBox([x1min,x1max,x2min,x2max,x3min,x3max,x4min,x4max,x5min,x5max]),fig1]), fig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Assess robustness by bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to assess the robustness of the estimated sensitivity indices, bootstrapping is performed (here we resample 100 times). The 95% confidence intervals of the sensitivity indices are plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sensitivity indices with confidence intervals using bootstrapping\n",
    "Nboot = 100\n",
    "# Warning: the following line may take some time to run, as the computation of\n",
    "# CDFs is costly:\n",
    "mvd_median, mvd_mean, mvd_max, spread_median, spread_mean, spread_max, idx, Yk = \\\n",
    "    Rg.RSA_indices_groups(X, Y, ngroup=10, Nboot=100)\n",
    "\n",
    "# Compute mean and confidence intervals of the sensitivity indices (mvd,\n",
    "# maximum vertical distance) across the bootstrap resamples:\n",
    "mvd_m, mvd_lb, mvd_ub = aggregate_boot(mvd_median) # shape (M,)\n",
    "\n",
    "mvd_m_p = pd.DataFrame(mvd_m,columns=[\"y\"])\n",
    "mvd_lb_p = pd.DataFrame(mvd_lb,columns=[\"y\"])\n",
    "mvd_ub_p = pd.DataFrame(mvd_ub,columns=[\"y\"])\n",
    "\n",
    "fig2 = px.scatter(mvd_m_p, x = [\"x1\",\"x2\", \"x3\"], y = 'y',color=['black','black','black'], width=600, height=400)\n",
    "fig2.add_scatter(y = mvd_lb_p.y, x = [\"x1\",\"x2\", \"x3\"], marker=dict(color=['red','blue','green']), mode = \"markers\")\n",
    "fig2.add_scatter(y = mvd_ub_p.y, x = [\"x1\",\"x2\", \"x3\"], marker=dict(color=['red','blue','green']), mode = \"markers\")\n",
    "\n",
    "fig2.layout.yaxis.range=[0,1]\n",
    "fig2.layout.yaxis.title=\"Maximum Vertical Distance\"\n",
    "fig2.layout.xaxis.title=\"\"\n",
    "fig2.update_traces(marker=dict(size=16),showlegend=False)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Visualise input factors interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to investigate the interactions between input factors we plot one input against the other, coloured by the value taken by the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inter = pd.concat([pd.DataFrame(X),pd.DataFrame(Y)],axis=1)\n",
    "data_inter.columns = ['x1', 'x2', 'x3', 'y']\n",
    "# index_vals = data_colscat['idx'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = go.Figure(data=go.Splom(\n",
    "                dimensions=[dict(label='x1',\n",
    "                                 values=data_inter['x1']),\n",
    "                            dict(label='x2',\n",
    "                                 values=data_inter['x2']),\n",
    "                            dict(label='x3',\n",
    "                                 values=data_inter['x3'])],\n",
    "                showupperhalf=False, # remove plots on diagonal\n",
    "                text=data_inter['y'],\n",
    "                marker=dict(color=data_inter['y'],colorbar=dict(title=\"Y\"),\n",
    "                            showscale=True, # colors encode categorical variables\n",
    "                            line_color='white', line_width=0.5,colorscale = px.colors.diverging.RdYlBu[::-1])\n",
    "                ))\n",
    "\n",
    "fig3.show()                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualise interactions between inputs through parallel coordinate plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = pd.concat([pd.DataFrame(X),pd.DataFrame(Y),pd.DataFrame(idx)],axis=1)\n",
    "data_group.columns = ['x1', 'x2', 'x3', 'y','idx']\n",
    "\n",
    "fig4 = px.parallel_coordinates(data_group, color=\"y\", dimensions=['x1','x2','x3','y'],\n",
    "                             color_continuous_scale=px.colors.diverging.RdYlBu[::-1],\n",
    "                             color_continuous_midpoint=2)\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"References\"></a>References\n",
    "\n",
    "\n",
    "RSA is based on the function created as part of the SAFE Toolbox by F. Pianosi, F. Sarrazin and T. Wagener at Bristol University (2015). Please refer to the `Licence` file in the SAFE toolbox. \n",
    "\n",
    "1) [SAFE Toolbox Website](https://www.safetoolbox.info/)\t\n",
    "\n",
    "2) [Introductory paper to SAFE - Pianosi et al. (2015)](https://www.sciencedirect.com/science/article/pii/S1364815215001188)\n",
    "\n",
    "3) [A review of available methods and workflows for Sensitivity Analysis - Pianosi et al. (2016)](https://www.sciencedirect.com/science/article/pii/S1364815216300287)\n",
    "\n",
    "4) [What has Global Sensitivity Analysis ever done for us? A systematic review to support scientific advancement and to inform policy-making in earth system modelling - Wagener and Pianosi (2019)](https://www.sciencedirect.com/science/article/pii/S0012825218300990)\n",
    "\n",
    "5) [Practical guide through the critical choices needed for Global Sensitivity Analysis - Noacco et al. (2019)](https://www.sciencedirect.com/science/article/pii/S2215016119302572?via%3Dihub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
